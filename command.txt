Command:

Virtualbox must be installed before minikube can start
Start the minikube for local
> minikube start
> minikube delete
> minikube status
> minikube start --driver=virtualbox
after the minikube is started, you can call the dashboard by
> minikube dashboard


Kube yaml file must have
1. apiVersion; 2. Kind; 3. metadata; 4. spec 
Yaml file indent can be by any number of spaces. 
It will work as long as it is indented in the same level. 
kubectl create -f <xxx.yml> (this will execute the yaml file in default namespace)

IMPORTANT!!!!!!!!
Reference Link:
https://kubernetes.io/docs/reference/kubectl/conventions/
The fastest way to generate a YAML TEMPLATE file
>kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml




check all the existing pods, services, replicaset, deployment
> kubectl get all

create pods with nginx
> kubectl run nginx --image=nginx

Check Pods Details
> kubectl get pods (kubectl get pod)
> kubectl get pods -o wide
> kubectl describe pod <pod-name>
  
delete pods
> kubectl delete pod <pod-name>
  
Update Pods
Update the pod-definition file and use 'kubectl apply' command or use 'kubectl edit pod <pod-name>' command.
  
GET / PUT / POST / DELETE is equivalence to get / edit / run /delete in k8s  
use edit / describe to check the details

Get Number of Nodes
> kubectl get nodes
> kubectl get nodes -o wide (show more details)


Check ReplicaSet
> kubectl get replicaset
> kubectl get rs
> kubectl describe replicaset <replicaset-name>
> kubectl describe pods (use to check error on creating the replicaset)

Delete ReplicaSet
> kubectl delete replicaset <replicaset-name>

Edit ReplicaSet; it will be automatically become effective after the file is edited and saved 
> kubectl edit replicaset <replica-set-name>

Scale (you can use kubectl edit rs <rs-name> to update the replicaset OR using the following command; they achievce the same goal)
> kubectl replace -f replicaset-definition.yml
> kubectl scale --replicas=6 -f replicaset-definition.yml
> kubectl scale --replicas=6 <type> <name> 
> kubectl scale --replicas=6 replicaset myapp-replicaset





Namespace

Namespace = House Mark Smith / Mark Williams; 
inside Smith's Home, you just call Mark then it automatically refer to Mark Smith. 
To call Mark Williams in Smith's Home, you need to call Mark Williams

There is a Default, kube-system, kube-public namespace
Or you can crate a Prod or Dev namespace 

The following is the example of DNS used in K8s
db-service.dev.svc.cluster.local
cluster.local is the default domain name used in k8s
svc is the service
dev is the name space
Db-service is the service name
If inside the same namespace, you can simply call "db-service"

> kubectl get namespace
> kubectl get pods --all-namespaces

> kubectl get pods (just get the pods in the default namespace)
> kubectl get pods --namespace=kube-system (specify the pods in other namespace)

> kubectl run redis --image=redis -n finance (create a redis image in namespace finance)

> kubectl create -f xxx.yml (create pod in default namespace)
> kubectl create -f xxx.yml --namespace=dev (create pod in dev namespace)
Or you can specify the namespace in the yaml file under "metadata: namespace:"







Deployment (allow rollout, rollback, pause, start)
> kubectl create -f deployment-definition.yml
> kubectl get deployments
you may also need to run the following; because deployment may create replicaset and pod
> kubectl get replicaset
> kubectl get pods
if you have a replicaset and deployment yml file, deployment will override replicaset
that's if you update the replicaset to 6 but the deployment is set to 1
then it will finally become 1, (that's you set replicaset to 6, but deployment will force it back to 1)
> kubectl scale --replicas=6 deployment/nginx
ususally deployment include replica set; replica set include pod (deployment > replica > pod)



Rollout
> kubectl rollout status deployment/my-app-deployment 
> kubectl rollout history deployment/my-app-deployment
deployment/my-app-deployment => type/deployment-name
 
Rollback
> kubectl rollout undo deployment/my-app-deployment


The following create a deployment with the name httpd-frontend, with image=httpd:2.4-alpine
then set the replicaset as 3
> kubectl create deployment httpd-frontend --image=httpd:2.4-alpine
> kubectl scale deployment --replicas=3 httpd-frontend


Apply a YAML file (this will create/update the config)
> kubectl apply -f <yaml file name>
assume you have a deployment with name "myapp-deployment" and you want to update the nginx image version from 1.7.1 to 1.9.1
> kubectl set image deployment/my-app-deployment nginx=nginx:1.9.1
after the above command, you can use the "> kubectl rollout history deployment/<name>" to check the rollout status (about the deployment)
or you can use "> kubectl rollout undo deployment/<name>" to roll back to the previous version


Check point Summary
Create:
> kubectl create -f deployment-definition.yml
Get:
> kubectl get deployments
Update:
> kubectl apply -f deployment-definition.yml
> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
Status:
> kubectl rollout status deployment/myapp-deployment
> kubectl rollout history deployment/myapp-deployment
Rollback:
> kubectl rollout undo deployment/myapp-deployment




Service (enable to communicate between difference components, service help to connect application together)
you can get the IP address of the kubernetes node by: (the internal private address is 10.244.0.0 and all pods are attached to it)
> kubectl get service

Service Type:
NodePort: service makes an internal POD accessible on a Port on the Node (use to listen to a port on the Node and forward reques on that port to a port on the POD running the web appliation.)
ClusterIP: the service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end servers to a set of backend servers.(group PODs together and provide a single interface to access the PODs in a group. eg. group of backend pods; the requests are forwarded to one of the PODs under the service randomly)
LoadBalancer: load balancer for the services
Summary: For a system, we will use the load balancer as the frist layer. Then we use NodePort to export the internal pods to load balancer. For internal pods, we use ClusterIP to connect them together.

TargetPort = Port in the Pod
Port = Port in the Service
NodePort = Port in Node (30000 - 32767)
Node <-> Service <-> Pod (Service connect the Node and Pod together)

create a service yaml file, use selector and label to connect the pod (detail refer to the example in /Service)

> kubectl describe service <service-name>
Create a service yaml file with name simple-webapp-deployment, with name web-service then map the port
> kubectl expose deployment simple-webapp-deployment --name=web-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml


Accessing the PODS through kubectl
> kubectl exec -it <port-name> -- bash



Show both of the pod and service together
> kubectl get pods, svc

to kick up the system, you can
1. run individual pod files + individual service files
2. run individual deployment files + individual service files





Scheduler
Scheduling is talking about putting a pod in a Node (set tolerate, affinity, resource in the pod; schedluer will base on this to put into node)
Label and selectors: you can add many labels as you want; uses selector to filter out base on the label
> kubectl get pods --selector key=value
> kubectl get all --selector env=prod,bu=finance,tier=frontend --no-headers | wc -l

You can assign a Pod by specify nodeName in the Pod Yaml file. The schedule will base on the node name to put the pod into a particular node.

Taints and Tolerations: Example, Tainted person make intolerant bugs go; Tainted person can't make tolerant bugs go (person=node, bug=pod)
If no taints and tolerate, the schedule will put the pod evenly across the node. 
If the node is tainted, only the scheduler can only place tolerated pod into this tainted node.
Taints and Tolerations doesn't ensure the tolerated pod can goto a particular tainted node. It just can tell which tolerated pods can come in.
> kubectl describe node node01 | grep -i taints (check a node if tainted)
> kubectl taint nodes node01 spray=mortein:NoSchedule (taint a node with key=spray value=mortein with effect=NoSchedule)
> kubectl taint nodes node1 key1=value1:NoSchedule

Taints the node by command
Tolerate the pod by edit the yaml file

It is impossible to memorise all the command, you need to know how to make use of document
https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

To remove the taint
> kubectl taint nodes node1 key1=value1:NoSchedule-


Node Selector: (use the label matching; Node affinity is the best practice on node selection; no advance/complex expression for node selector)
In the POD: use "nodeSelector: size: Large", define it in yaml file
In the Node: kubectl label nodes node-1 size=Large
> kubectl get node <node-name> --show-labels
> kubectl label node node01 color=blue (can add label to the node at anytime)

Node Affinity: (ensure the pods host in particular node; can have a more complex solution then node selector)
Add label to node
Add affinity to pod's yaml

Node Affinity + Taint and Tolerate always work together (watch Udemy Section 64)

Set Pod Resource in yaml file

Daemon Sets: will ensure one copy of the pod is always present in all nodes (eg. Logs)
Daemon sets yaml file with "kind: DaemonSet", it will specify which container to be the daemon set
> kubectl get daemonsets --all-namespaces (daemonsets command is similar to deployment, replicas,)


Static Pod (is talking about no control plane, no controller, no scheduler, etc. only have the pod itself)
The kubelet inside the pod will check the defined path to read the YAML file
It can just run on Pod level, therefore the YAML file can't have any deployment, replica elements
The details is listed in kubelet.service
Static pod definition files are stored in /etc/kubenetes/manifests (can be change, depends on the config)
After it's created you can't use the kubectl command, you should use docker command instant
Because currently no kube api server is there
Usually used to define the control plane
Example to create a static pod
> kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

Multiple Scheduler (k8s can have multiple schedulers, the pod definition file can specify which scheduler to use)
Can use kube-scheduler.service to define the additional scheduler (Udemy Section 76)






Logging
Monitor Cluster Components
To monitor the cluster, we need to have a third party log matrix tool
> git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
After download the above git, enter the folder and install all the yaml files by
> kubectl create -f .
Then run
> kubectl top node
> kubectl top pod --sort-by='memory'

For logging a docker:
> dockers logs -f <docker-name> (the -f option will make the log run in live)
For logging a Pod (one pod may have many containers)
> kubectl logs -f <pod-name> <container-name> (for multiple container)
> kubectl logs -f <pod-name> (for single container)







Docker Volume vs Docker Bind Mode
Docker Volume: use the docker default host path
Docker Bind Mode: map to a specified host path

Docker Volume:
Method 1:
> docker volume create data_volume (this will create a data_volume under the docker default path /var/lib/docker/volumes/)
> docker run -v data_volume:/var/lib/mysql mysql (this will map docker path /var/lib/mysql to host path /var/lib/docker/volumes/data_volume)
Method 2:
> docker run -v data_volume:/var/lib/mysql mysql (if you don't enter the "docker volume create" command, this will auto generate the host path "/var/lib/docker/volumes/data_volume" and map to container path "/var/lib/mysql")

Docker Bind Mode:  (fix a volume to a pod, not practical if there are a lot of pod sharing the volume)
Method 1:
> docker run -v /data/mysql:/var/lib/mysql mysql (/data/mysql is the host path, /var/lib/mysql is the container path; it auto become the bind mode because path is specified)
Method 2: (a newer method than method 1)
> docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql (same results as Method 1 but in a more modern way to achieve it)



In Kubernetes:
You can define the host volume and container volume inside the YAML file
=====================================================
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
=====================================================


Persistent Volume (tell the specification of the volume) and Persistent Volume Claim (tell the requirement of the volume)
K8s will try to fit the persistent volume claim to the right persistent volume
You need to tell the pod to use the persistent volume claim (pvc), k8s will map pvc to pv according to the resource requirement 
=====================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
=====================================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
=====================================================
> kubectl get pvc (PVC = persistent volume claim)
> kubectl get pv (PV = persistent volume)


Apply the persistent volume to the pod
=====================================================
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
=====================================================
